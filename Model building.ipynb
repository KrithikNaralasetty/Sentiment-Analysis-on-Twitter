{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "csv = 'clean_tweet.csv'\n",
    "my_df = pd.read_csv(csv,index_col=0)\n",
    "my_df['target'].replace(4,1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1596522 entries, 0 to 1596521\n",
      "Data columns (total 2 columns):\n",
      "text      1596522 non-null object\n",
      "target    1596522 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n"
     ]
    }
   ],
   "source": [
    "my_df.dropna(inplace=True)\n",
    "my_df.reset_index(drop=True,inplace=True)\n",
    "my_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 1564591 entries with 50.00% negative, 50.00% positive\n",
      "Validation set has total 15965 entries with 50.00% negative, 50.00% positive\n",
      "Test set has total 15966 entries with 50.39% negative, 49.61% positive\n"
     ]
    }
   ],
   "source": [
    "x = my_df.text\n",
    "y = my_df.target\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\n",
    "print (\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),(len(x_train[y_train == 0]) / (len(x_train)*1.))*100,(len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print (\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),(len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,(len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print (\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),(len(x_test[y_test == 0]) / (len(x_test)*1.))*100,(len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:648: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = pd.concat([x_train,x_validation,x_test])\n",
    "all_x_w2v = labelize_tweets_ug(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1949807.68it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_bg_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_bg_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1852828.10it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2125680.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2091132.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2005832.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2105023.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2007002.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2032972.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2102682.91it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2047340.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1645472.23it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1711379.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1640785.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1628242.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1853610.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2070299.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1616922.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1999493.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1809292.59it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1854244.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2066631.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1758064.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:01<00:00, 1283679.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1920536.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1883800.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1735249.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2116700.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1802008.23it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1750971.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2088834.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2116896.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_bg_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_bg_cbow.alpha -= 0.002\n",
    "    model_bg_cbow.min_alpha = model_ug_cbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1687562.50it/s]\n"
     ]
    }
   ],
   "source": [
    "model_bg_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_bg_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2100593.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1896241.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2104239.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1666787.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2173367.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1760029.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1726205.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1911336.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1702325.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1690325.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1669045.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2044643.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1868102.30it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1775280.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1658566.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1956619.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:01<00:00, 1500164.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2088121.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2026408.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1652628.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1636048.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 2123287.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1845178.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1783070.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1638208.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1996598.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1812026.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1652671.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:01<00:00, 1327036.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 1596522/1596522 [00:00<00:00, 1736615.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_bg_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_bg_sg.alpha -= 0.002\n",
    "    model_bg_sg.min_alpha = model_ug_sg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow.save('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg.save('w2v_model_ug_sg.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_ug_cbow = KeyedVectors.load('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg = KeyedVectors.load('w2v_model_ug_sg.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109094"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_ug_cbow.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 109094 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268091"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1564591, 45)\n"
     ]
    }
   ],
   "source": [
    "x_train_seq = pad_sequences(sequences, maxlen=45)\n",
    "print('Shape of data tensor:', x_train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,  4989,    90,   582,   115],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,   141,    39,     7,   398,    14,\n",
       "          200,    63,    33,     6,  1011,    83,     6,   184,   683],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,   108,\n",
       "        10622, 16148,   255,  2950,     5,  1267,  1070,     2,    64],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     6,   112,   995,    71,    23,\n",
       "           55,  7794,  1456,    76,   269,  2121,   195,    10,   389],\n",
       "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     4,   462]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_seq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = tokenizer.texts_to_sequences(x_validation)\n",
    "x_val_seq = pad_sequences(sequences_val, maxlen=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1564591 samples, validate on 15965 samples\n",
      "Epoch 1/5\n",
      " - 9796s - loss: 0.4228 - accuracy: 0.8052 - val_loss: 0.3974 - val_accuracy: 0.8180\n",
      "Epoch 2/5\n",
      " - 9500s - loss: 0.3780 - accuracy: 0.8306 - val_loss: 0.3976 - val_accuracy: 0.8199\n",
      "Epoch 3/5\n",
      " - 9516s - loss: 0.3402 - accuracy: 0.8506 - val_loss: 0.4102 - val_accuracy: 0.8138\n",
      "Epoch 4/5\n",
      " - 9509s - loss: 0.2960 - accuracy: 0.8727 - val_loss: 0.4513 - val_accuracy: 0.8087\n",
      "Epoch 5/5\n",
      " - 9509s - loss: 0.2527 - accuracy: 0.8937 - val_loss: 0.4891 - val_accuracy: 0.8042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17bc69e1f48>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ptw2v = Sequential()\n",
    "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=45, trainable=True)\n",
    "model_ptw2v.add(e)\n",
    "model_ptw2v.add(Flatten())\n",
    "model_ptw2v.add(Dense(256, activation='relu'))\n",
    "model_ptw2v.add(Dense(1, activation='sigmoid'))\n",
    "model_ptw2v.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_ptw2v.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_validation), epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 45, 200)           20000000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 44, 100)           40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "=================================================================\n",
      "Total params: 20,040,100\n",
      "Trainable params: 20,040,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "structure_test = Sequential()\n",
    "e = Embedding(100000, 200, input_length=45)\n",
    "structure_test.add(e)\n",
    "structure_test.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "structure_test.add(GlobalMaxPooling1D())\n",
    "structure_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1564591 samples, validate on 15965 samples\n",
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "filepath=\"CNN_best_weights.{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=45, trainable=True)\n",
    "model.add(e)\n",
    "model.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train_seq, y_train, batch_size=32, epochs=5,validation_data=(x_val_seq, y_validation), callbacks = [checkpoint],verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
